import discord, openai, random, time, asyncio
from discord.ext import commands

class Gen(commands.Cog):
    """ Prompt generation using OpenAI, replies to user command, edits reply live """

    def __init__(self, bot):
        self.bot = bot
                    
    @commands.Cog.listener()
    async def on_ready(self):
        print('prompt_generation.py online.')
    
    @commands.command()
    async def prompt(self, ctx):
    
        if ctx.message.content.replace("!prompt", "") == "":
            await ctx.reply("The '!prompt' command is used for text generation.")
            return
        
        full_message = "" #contains full text
        curr_message = "" #current text of the current message
        final_response = ""
        curr_response = None #the current discord message
        completions = [] #list representation of the total completions
        curr_completions = [] #current message list representation of completions
        print("\nAuthor: " + str(ctx.author))
        print("\nMessage prompt: " + str(ctx.message.content))

        try:
            while full_message == "": # get first response, wait while doing so
                p = ctx.message.content.replace("!prompt", "")
                response = await exponential_wait(ctx, p)
                full_message = response.choices[0].text
                if full_message != "":
                    break
                    
            if ctx.message.content and (full_message != ""):
                curr_response = await ctx.reply(full_message)
            
            completions.append(response.choices[0].text)
            curr_completions.append(response.choices[0].text)
            
            while ("<EOP>" not in full_message) and ("<EOL>" not in full_message):
                curr_prompt = ctx.message.content.replace("!prompt", "") + full_message
                response = await exponential_wait(ctx, curr_prompt)
                
                if response.choices[0].text == "":
                    break
                    
                if len(str(curr_message + response.choices[0].text)) > 2000 or (len("".join(curr_completions)) > 2000):
                    curr_message = "".join(curr_completions)
                    curr_message = curr_message[:2000]
                    await curr_response.edit(content=curr_message)
                    curr_completions = []
                    curr_message = response.choices[0].text
                    temp_response = await curr_response.reply(curr_message)
                    curr_response = temp_response
                    
                else:
                    curr_message = curr_message + response.choices[0].text
                    
                curr_completions.append(response.choices[0].text)
                completions.append(response.choices[0].text)
                full_message = full_message + response.choices[0].text
                
                if ctx.message.content and (curr_message != "") and (len("".join(curr_completions)) < 2000):
                    await curr_response.edit(content="".join(curr_completions))

        except Exception as e:
            print(e)
            if curr_response:
                await curr_response.edit(content=e)
            final_response = "".join(completions)
            error_response = await ctx.reply("Sorry, I could not finish your prompt due to this error:\n\n" + str(e)) 
            print(str(e))
            
        finally:
            curr_message = "".join(curr_completions)
            curr_message = curr_message[:2000]
            final_response = "".join(completions)
            print("\nFinal response:" + str(final_response))
            if curr_response:
                await curr_response.edit(content=curr_message)

async def setup(bot):
    await bot.add_cog(Gen(bot))
    print("prompt cog successfully loaded.")

def generate_response(ctx, p):
    """
    Function which calls the OpenAI API completion function.
    Returns the next token generated by the API.
    """
    return openai.Completion.create(
                engine="text-davinci-003",
                prompt=p,
                temperature=1)

async def exponential_wait(ctx, p):
        """
        Wrapper function that implements the OpenAI API prompt completion with exponential cooldown when the rate limit is reached.
        Parameters are the discord context (ctx), and the string prompt itself (p).
        Assume p has been cleaned to remove the command invocation.
        If the API request rate is reached, the exception is handled with a random timer.
        Timer increases exponentially with each round until it hits the limit.
        """
        retries = 0
        max_retries = 12
        init_delay = 1
        delay = init_delay
        exp_base = 2
        
        while True:
            try:
                return(generate_response(ctx, p))
                
            except openai.error.RateLimitError as e:
                retries += 1
                
                if retries >= max_retries:
                    raise Exception("Sorry! Maximum amount of rate limit retry cycles of this prompt has been reached. Try again later!")
                    
                delay *= exp_base * (1 + random.random())
                await ctx.reply("Rate limit reached, next try in " + str(round(delay, 3)) + " seconds, " + str(retries) + "/12 allowed retries attempted.",delete_after=(delay+1))
                print("Rate limit reached, next try in " + str(delay) + " seconds, current cycle: " + str(retries))
                await asyncio.sleep(delay)
                
            except Exception as e:
                    raise e